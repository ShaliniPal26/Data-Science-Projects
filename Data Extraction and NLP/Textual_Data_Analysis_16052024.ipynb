{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e226166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a405001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    text = \"\"  # Initialize text\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        page.raise_for_status()\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Handling for article text\n",
    "        text_element = soup.find(\"div\", {'class': 'td-post-content'})\n",
    "    \n",
    "        if text_element:\n",
    "            text = text_element.get_text()\n",
    "        else:\n",
    "            print(\"Text not found on the page.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the page: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    return text\n",
    "\n",
    "def save_to_text_file(url_id, text):\n",
    "    file_name = f\"{url_id}.txt\"\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Text:\\n{text}\")\n",
    "\n",
    "def save_to_text_file(url_id, text):\n",
    "    folder_name = \"TextFiles\"\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    file_name = os.path.join(folder_name, f\"{url_id}.txt\")\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Text:\\n{text}\")\n",
    "\n",
    "    print(f\"Text saved to: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3909f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: TextFiles\\blackassign0001.txt\n",
      "Article for blackassign0001 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0002.txt\n",
      "Article for blackassign0002 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0003.txt\n",
      "Article for blackassign0003 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0004.txt\n",
      "Article for blackassign0004 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0005.txt\n",
      "Article for blackassign0005 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0006.txt\n",
      "Article for blackassign0006 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0007.txt\n",
      "Article for blackassign0007 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0008.txt\n",
      "Article for blackassign0008 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0009.txt\n",
      "Article for blackassign0009 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0010.txt\n",
      "Article for blackassign0010 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0011.txt\n",
      "Article for blackassign0011 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0012.txt\n",
      "Article for blackassign0012 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0013.txt\n",
      "Article for blackassign0013 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0014.txt\n",
      "Article for blackassign0014 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0015.txt\n",
      "Article for blackassign0015 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0016.txt\n",
      "Article for blackassign0016 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0017.txt\n",
      "Article for blackassign0017 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0018.txt\n",
      "Article for blackassign0018 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0019.txt\n",
      "Article for blackassign0019 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0020.txt\n",
      "Article for blackassign0020 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0021.txt\n",
      "Article for blackassign0021 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0022.txt\n",
      "Article for blackassign0022 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0023.txt\n",
      "Article for blackassign0023 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0024.txt\n",
      "Article for blackassign0024 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0025.txt\n",
      "Article for blackassign0025 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0026.txt\n",
      "Article for blackassign0026 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0027.txt\n",
      "Article for blackassign0027 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0028.txt\n",
      "Article for blackassign0028 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0029.txt\n",
      "Article for blackassign0029 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0030.txt\n",
      "Article for blackassign0030 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0031.txt\n",
      "Article for blackassign0031 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0032.txt\n",
      "Article for blackassign0032 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0033.txt\n",
      "Article for blackassign0033 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0034.txt\n",
      "Article for blackassign0034 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0035.txt\n",
      "Article for blackassign0035 extracted and saved.\n",
      "Error fetching the page: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Failed to extract article for blackassign0036.\n",
      "Text saved to: TextFiles\\blackassign0037.txt\n",
      "Article for blackassign0037 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0038.txt\n",
      "Article for blackassign0038 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0039.txt\n",
      "Article for blackassign0039 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0040.txt\n",
      "Article for blackassign0040 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0041.txt\n",
      "Article for blackassign0041 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0042.txt\n",
      "Article for blackassign0042 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0043.txt\n",
      "Article for blackassign0043 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0044.txt\n",
      "Article for blackassign0044 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0045.txt\n",
      "Article for blackassign0045 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0046.txt\n",
      "Article for blackassign0046 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0047.txt\n",
      "Article for blackassign0047 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0048.txt\n",
      "Article for blackassign0048 extracted and saved.\n",
      "Error fetching the page: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Failed to extract article for blackassign0049.\n",
      "Text saved to: TextFiles\\blackassign0050.txt\n",
      "Article for blackassign0050 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0051.txt\n",
      "Article for blackassign0051 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0052.txt\n",
      "Article for blackassign0052 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0053.txt\n",
      "Article for blackassign0053 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0054.txt\n",
      "Article for blackassign0054 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0055.txt\n",
      "Article for blackassign0055 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0056.txt\n",
      "Article for blackassign0056 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0057.txt\n",
      "Article for blackassign0057 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0058.txt\n",
      "Article for blackassign0058 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0059.txt\n",
      "Article for blackassign0059 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0060.txt\n",
      "Article for blackassign0060 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0061.txt\n",
      "Article for blackassign0061 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0062.txt\n",
      "Article for blackassign0062 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0063.txt\n",
      "Article for blackassign0063 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0064.txt\n",
      "Article for blackassign0064 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0065.txt\n",
      "Article for blackassign0065 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0066.txt\n",
      "Article for blackassign0066 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0067.txt\n",
      "Article for blackassign0067 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0068.txt\n",
      "Article for blackassign0068 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0069.txt\n",
      "Article for blackassign0069 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0070.txt\n",
      "Article for blackassign0070 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0071.txt\n",
      "Article for blackassign0071 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0072.txt\n",
      "Article for blackassign0072 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0073.txt\n",
      "Article for blackassign0073 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0074.txt\n",
      "Article for blackassign0074 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0075.txt\n",
      "Article for blackassign0075 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0076.txt\n",
      "Article for blackassign0076 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0077.txt\n",
      "Article for blackassign0077 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0078.txt\n",
      "Article for blackassign0078 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0079.txt\n",
      "Article for blackassign0079 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0080.txt\n",
      "Article for blackassign0080 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0081.txt\n",
      "Article for blackassign0081 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0082.txt\n",
      "Article for blackassign0082 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0083.txt\n",
      "Article for blackassign0083 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0084.txt\n",
      "Article for blackassign0084 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0085.txt\n",
      "Article for blackassign0085 extracted and saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: TextFiles\\blackassign0086.txt\n",
      "Article for blackassign0086 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0087.txt\n",
      "Article for blackassign0087 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0088.txt\n",
      "Article for blackassign0088 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0089.txt\n",
      "Article for blackassign0089 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0090.txt\n",
      "Article for blackassign0090 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0091.txt\n",
      "Article for blackassign0091 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0092.txt\n",
      "Article for blackassign0092 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0093.txt\n",
      "Article for blackassign0093 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0094.txt\n",
      "Article for blackassign0094 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0095.txt\n",
      "Article for blackassign0095 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0096.txt\n",
      "Article for blackassign0096 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0097.txt\n",
      "Article for blackassign0097 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0098.txt\n",
      "Article for blackassign0098 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0099.txt\n",
      "Article for blackassign0099 extracted and saved.\n",
      "Text saved to: TextFiles\\blackassign0100.txt\n",
      "Article for blackassign0100 extracted and saved.\n",
      "Extraction process completed.\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "excel_file = \"Input.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Extract article text\n",
    "    text = extract_article_text(url)\n",
    "\n",
    "    # Save to text file\n",
    "    if text:\n",
    "        save_to_text_file(url_id, text)\n",
    "        print(f\"Article for {url_id} extracted and saved.\")\n",
    "    else:\n",
    "        print(f\"Failed to extract article for {url_id}.\")\n",
    "\n",
    "print(\"Extraction process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829d4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['URL_ID'].isin(['blackassign0036', 'blackassign0049'])]\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# dropping blackassign0036 and blackassign0049 as the pages are not found or present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29aa34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c58ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    cleaned_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return cleaned_words\n",
    "\n",
    "def load_words_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return set(words)\n",
    "\n",
    "# Load positive and negative words from files\n",
    "positive_words = load_words_from_file('positive-words.txt')\n",
    "negative_words = load_words_from_file('negative-words.txt')\n",
    "\n",
    "def calculate_sentiment_scores(text):\n",
    "    words = clean_and_tokenize(text)\n",
    "    \n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score) / max((positive_score + negative_score), 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / max((len(words) + 0.000001), 1.0)\n",
    "    \n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "    \n",
    "    \n",
    "# Function to calculate readability metrics\n",
    "def calculate_readability_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = clean_and_tokenize(text)\n",
    "    \n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    percentage_complex_words = sum(1 for word in words if len(word) > 2) / len(words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    return avg_sentence_length, percentage_complex_words, fog_index\n",
    "\n",
    "# Function to calculate average number of words per sentence\n",
    "def average_words_per_sentence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    cleaned_text = clean_and_tokenize(text) \n",
    "    return len(cleaned_text) / len(sentences)\n",
    "\n",
    "# Function to calculate complex word count\n",
    "def calculate_complex_word_count(text):\n",
    "    words = clean_and_tokenize(text)\n",
    "    return sum(1 for word in words if len(word) > 2)\n",
    "\n",
    "# Function to calculate word count\n",
    "def calculate_word_count(text):\n",
    "    return len(clean_and_tokenize(text))\n",
    "\n",
    "# Function to calculate syllable count per word\n",
    "def calculate_syllable_per_word(text):\n",
    "    words = clean_and_tokenize(text)\n",
    "    syllable_count = sum(len(re.findall(r'[aeiou]+', word)) for word in words)\n",
    "    return syllable_count / max(1, len(words))  # Avoid division by zero\n",
    "\n",
    "# Function to count personal pronouns\n",
    "def count_personal_pronouns(text):\n",
    "    personal_pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, flags=re.IGNORECASE)\n",
    "    return len(personal_pronouns)\n",
    "\n",
    "# Function to calculate average word length\n",
    "def calculate_average_word_length(text):\n",
    "    words = clean_and_tokenize(text)\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "    return total_chars / max(1, len(words))  # Avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa1feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'TextFiles/'\n",
    "file_names = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9093fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "avg_sentence_length = []\n",
    "percentage_complex_words = []\n",
    "fog_index = []\n",
    "avg_words_per_sentence = []\n",
    "complex_word_count = []\n",
    "word_count = []\n",
    "syllable_per_word = []\n",
    "personal_pronouns_count = []\n",
    "avg_word_length = []\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Example usage of analysis functions\n",
    "    p_score, n_score, pol_score, subj_score = calculate_sentiment_scores(text)\n",
    "    positive_score.append(p_score)\n",
    "    negative_score.append(n_score)\n",
    "    polarity_score.append(pol_score)\n",
    "    subjectivity_score.append(subj_score)\n",
    "        \n",
    "    avg_sent_length, percent_comp_words, f_index = calculate_readability_metrics(text)\n",
    "    avg_sentence_length.append(avg_sent_length)\n",
    "    percentage_complex_words.append(percent_comp_words)\n",
    "    fog_index.append(f_index)\n",
    "    \n",
    "    avg_words_per_sent = average_words_per_sentence(text)\n",
    "    avg_words_per_sentence.append(avg_words_per_sent)\n",
    "    \n",
    "    c_word_count = calculate_complex_word_count(text)\n",
    "    complex_word_count.append(c_word_count)\n",
    "    \n",
    "    w_count = calculate_word_count(text)\n",
    "    word_count.append(w_count)\n",
    "    \n",
    "    syl_per_word = calculate_syllable_per_word(text)\n",
    "    syllable_per_word.append(syl_per_word)\n",
    "    \n",
    "    per_pronouns_count = count_personal_pronouns(text)\n",
    "    personal_pronouns_count.append(per_pronouns_count)\n",
    "    \n",
    "    avg_w_length = calculate_average_word_length(text)\n",
    "    avg_word_length.append(avg_w_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4793246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ebd35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data['URL_ID'] = df['URL_ID']\n",
    "output_data['URL'] = df['URL']\n",
    "output_data['POSITIVE_SCORE'] = positive_score\n",
    "output_data['NEGATIVE_SCORE'] = negative_score\n",
    "output_data['POLARITY_SCORE'] = polarity_score\n",
    "output_data['SUBJECTIVITY_SCORE'] = subjectivity_score\n",
    "output_data['AVG SENTENCE LENGTH'] = avg_sentence_length\n",
    "output_data['PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n",
    "output_data['FOG INDEX'] = fog_index\n",
    "output_data['AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sentence\n",
    "output_data['COMPLEX WORD COUNT'] = complex_word_count\n",
    "output_data['WORD COUNT'] = word_count\n",
    "output_data['SYLLABLE PER WORD'] = syllable_per_word\n",
    "output_data['PERSONAL PRONOUNS'] = personal_pronouns_count\n",
    "output_data['AVG WORD LENGTH'] = avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7937bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE_SCORE</th>\n",
       "      <th>NEGATIVE_SCORE</th>\n",
       "      <th>POLARITY_SCORE</th>\n",
       "      <th>SUBJECTIVITY_SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.080386</td>\n",
       "      <td>7.974359</td>\n",
       "      <td>0.996785</td>\n",
       "      <td>3.588457</td>\n",
       "      <td>7.974359</td>\n",
       "      <td>620</td>\n",
       "      <td>622</td>\n",
       "      <td>2.110932</td>\n",
       "      <td>12</td>\n",
       "      <td>6.353698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>64</td>\n",
       "      <td>31</td>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.112028</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>0.995283</td>\n",
       "      <td>4.638113</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>844</td>\n",
       "      <td>848</td>\n",
       "      <td>2.479953</td>\n",
       "      <td>6</td>\n",
       "      <td>7.216981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.098918</td>\n",
       "      <td>11.350877</td>\n",
       "      <td>0.996909</td>\n",
       "      <td>4.939114</td>\n",
       "      <td>11.350877</td>\n",
       "      <td>645</td>\n",
       "      <td>647</td>\n",
       "      <td>2.806801</td>\n",
       "      <td>13</td>\n",
       "      <td>8.055641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>39</td>\n",
       "      <td>75</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>0.173780</td>\n",
       "      <td>12.615385</td>\n",
       "      <td>0.995427</td>\n",
       "      <td>5.444325</td>\n",
       "      <td>12.615385</td>\n",
       "      <td>653</td>\n",
       "      <td>656</td>\n",
       "      <td>2.603659</td>\n",
       "      <td>5</td>\n",
       "      <td>7.821646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.078086</td>\n",
       "      <td>10.179487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.471795</td>\n",
       "      <td>10.179487</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>2.337531</td>\n",
       "      <td>6</td>\n",
       "      <td>7.128463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE_SCORE  NEGATIVE_SCORE  POLARITY_SCORE  SUBJECTIVITY_SCORE  \\\n",
       "0              44               6        0.760000            0.080386   \n",
       "1              64              31        0.347368            0.112028   \n",
       "2              40              24        0.250000            0.098918   \n",
       "3              39              75       -0.315789            0.173780   \n",
       "4              23               8        0.483871            0.078086   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0             7.974359                     0.996785   3.588457   \n",
       "1            10.600000                     0.995283   4.638113   \n",
       "2            11.350877                     0.996909   4.939114   \n",
       "3            12.615385                     0.995427   5.444325   \n",
       "4            10.179487                     1.000000   4.471795   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                          7.974359                 620         622   \n",
       "1                         10.600000                 844         848   \n",
       "2                         11.350877                 645         647   \n",
       "3                         12.615385                 653         656   \n",
       "4                         10.179487                 397         397   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0           2.110932                 12         6.353698  \n",
       "1           2.479953                  6         7.216981  \n",
       "2           2.806801                 13         8.055641  \n",
       "3           2.603659                  5         7.821646  \n",
       "4           2.337531                  6         7.128463  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f310a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data.to_excel('Output_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b623b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
